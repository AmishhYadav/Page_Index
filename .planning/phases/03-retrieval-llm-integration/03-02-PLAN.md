# Plan 03-02: Implement LLM Orchestration Service

## Goal
Implement `LLMService` to generate citation-backed answers using retrieved context.

<tasks>
  <task id="1" title="Setup LLM Client">
    <description>Update `requirements.txt` with necessary LLM client (e.g., `openai` or `google-generativeai`). Update `app/core/config.py` for API keys.</description>
  </task>
  <task id="2" title="Create LLMService">
    <description>Create `app/services/llm.py`. Implement `generate_answer(query, context)`:
    1. Construct a system prompt that mandates citations.
    2. Format the context list into a readable prompt segment.
    3. Call the LLM and return the text response.</description>
  </task>
  <task id="3" title="Mock Verification">
    <description>Create `test_03_02.py` with a mock LLM provider to verify prefix/prompt construction logic.</description>
  </task>
</tasks>

<verification>
1. Generated prompt includes all context snippets and their source labels.
2. System instructions for citations are present in the final prompt payload.
</verification>

<must_haves>
- Clear separation between user query and retrieval context in the prompt.
- Handled LLM API errors gracefully (logs and standard exceptions).
</must_haves>
