# Plan 04-04: Context Window Packing Optimization

## Goal
Replace naive context concatenation with a budget-aware packer that maximizes information density while respecting LLM token limits.

<tasks>
  <task id="1" title="Add Dependencies">
    <description>Add `tiktoken` to `requirements.txt`. Add `CONTEXT_TOKEN_BUDGET` and `MIN_RELEVANCE_THRESHOLD` to `app/core/config.py`.</description>
  </task>
  <task id="2" title="Implement ContextPacker">
    <description>Create `app/services/context_packer.py`. Implement `ContextPacker.pack(results, token_budget)`:
    1. Estimate tokens per section using `tiktoken`.
    2. Drop sections below `MIN_RELEVANCE_THRESHOLD`.
    3. Deduplicate adjacent sections from the same (document, page).
    4. Greedily pack sections until budget is exhausted.
    5. Return the packed list and a `packed_metadata` summary.</description>
  </task>
  <task id="3" title="Integrate into LLMService">
    <description>Update `app/services/llm.py` to call `ContextPacker.pack()` before `format_context()`. Log the number of sections dropped for budget or threshold reasons.</description>
  </task>
  <task id="4" title="Verification">
    <description>Write a test with 20 mock sections (varying lengths). Assert that the packer respects the token budget and drops low-relevance items first.</description>
  </task>
</tasks>

<verification>
1. Total token count of packed context is always â‰¤ `CONTEXT_TOKEN_BUDGET`.
2. Low-relevance sections are dropped before high-relevance ones.
3. Deduplication merges same-page sections into a single block.
</verification>

<must_haves>
- Token estimation must be model-aware (different tokenizers for different LLMs).
- Packing metadata logged for observability.
- Budget of 0 means unlimited (backward compatible).
</must_haves>
