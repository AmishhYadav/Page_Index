# Plan 04-05: Deterministic Citation Enforcement

## Goal
Replace soft prompt-based citation requests with a deterministic validator that guarantees every citation in the LLM response maps to a real source.

<tasks>
  <task id="1" title="Switch to Structured Output">
    <description>Update `app/services/llm.py` to request JSON-mode output from the LLM. Define the expected schema:
    ```json
    {
      "answer": "string",
      "inline_citations": [
        {"source_index": 0, "claim": "string"}
      ]
    }
    ```
    Map `source_index` back to the context list.</description>
  </task>
  <task id="2" title="Implement CitationValidator">
    <description>Create `app/services/citation_validator.py`. Implement `validate(llm_output, context_sources)`:
    1. Parse all `[Document: X, Page Y]` patterns from the answer text.
    2. Cross-check each against the actual context sources provided.
    3. Flag or strip unverifiable citations.
    4. Return a validated response with a `citation_integrity` score.</description>
  </task>
  <task id="3" title="Update Response Schema">
    <description>Update `app/schemas/ask.py` to include:
    - `citation_integrity: float` (0.0–1.0)
    - `unverified_citations: List[str]`</description>
  </task>
  <task id="4" title="Integrate and Verify">
    <description>Wire the validator into the `/ask` pipeline after LLM generation. Write a test with a mock LLM that produces both valid and invalid citations. Assert that invalid ones are flagged.</description>
  </task>
</tasks>

<verification>
1. Valid citations pass validation and appear in the response.
2. Fabricated citations are flagged and appear in `unverified_citations`.
3. `citation_integrity` is 1.0 when all citations are valid.
</verification>

<must_haves>
- Regex-based parsing for `[Document: X, Page Y]` format.
- Structured output fallback: if JSON parsing fails, fall back to regex extraction.
- Backward compatible — old responses without structured output still work.
</must_haves>
